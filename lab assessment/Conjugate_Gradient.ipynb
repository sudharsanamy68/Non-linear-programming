{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Gradient of f: [ 1 -1]\n",
      "Search direction S: [-1  1]\n",
      "X: [-1.  1.]\n",
      "Lambda: 1.0\n",
      "Iteration 2\n",
      "Gradient of f: [-1. -1.]\n",
      "Search direction S: [0. 2.]\n",
      "X: [-1.  2.]\n",
      "Lambda: 0.5\n",
      "Optimal solution: [-1.  2.]\n",
      "Minimum value of the objective function: -1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x1, x2):\n",
    "    return x1 - x2 + 2*x1**2 + 2*x1*x2 + x2**2\n",
    "\n",
    "def gradient_f(x1, x2):\n",
    "    df_dx1 = 1 + 4*x1 + 2*x2\n",
    "    df_dx2 = -1 + 2*x1 + 2*x2\n",
    "    return np.array([df_dx1, df_dx2])\n",
    "\n",
    "def optimal_step_length(X, S):\n",
    "    gradient = gradient_f(*X)\n",
    "    numerator = np.dot(gradient, S)\n",
    "    denominator = np.dot(S, S)\n",
    "    return -numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def fletcher_reeves(X0, epsilon=1e-4, max_iterations=100):\n",
    "    X = X0\n",
    "    for i in range(max_iterations):\n",
    "        gradient = gradient_f(*X)\n",
    "        if i == 0:\n",
    "            S = -gradient\n",
    "        else:\n",
    "            beta = np.dot(gradient, gradient) / np.dot(gradient_prev, gradient_prev)\n",
    "            S = -gradient + beta * S_prev\n",
    "        lambda_star = optimal_step_length(X, S)\n",
    "        X_new = X + lambda_star * S\n",
    "        print(\"Iteration\", i + 1)\n",
    "        print(\"Gradient of f:\", gradient)\n",
    "        print(\"Search direction S:\", S)\n",
    "        print(\"X:\", X_new)\n",
    "        print(\"Lambda:\", lambda_star)\n",
    "        if i == 1 or np.linalg.norm(X_new - X) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            gradient_prev = gradient\n",
    "            S_prev = S\n",
    "            X = X_new\n",
    "    return X_new\n",
    "\n",
    "# Initial point\n",
    "X0 = np.array([0, 0])\n",
    "\n",
    "# Perform optimization\n",
    "optimal_solution = fletcher_reeves(X0)\n",
    "print(\"Optimal solution:\", optimal_solution)\n",
    "print(\"Minimum value of the objective function:\", f(*optimal_solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Gradient of f: [4 4]\n",
      "Search direction S: [-4 -4]\n",
      "X: [-3. -2.]\n",
      "Lambda: 1.0\n",
      "Iteration 2\n",
      "Gradient of f: [-12.  -4.]\n",
      "Search direction S: [ -8. -16.]\n",
      "X: [1. 6.]\n",
      "Lambda: -0.5\n",
      "Optimal solution: [1. 6.]\n",
      "Minimum value of the objective function: 38.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x1, x2):\n",
    "    return 2 * x1**2 + x2**2\n",
    "\n",
    "def gradient_f(x1, x2):\n",
    "    df_dx1 = 4 * x1\n",
    "    df_dx2 = 2 * x2\n",
    "    return np.array([df_dx1, df_dx2])\n",
    "\n",
    "def optimal_step_length(X, S):\n",
    "    gradient = gradient_f(*X)\n",
    "    numerator = np.dot(gradient, S)\n",
    "    denominator = np.dot(S, S)\n",
    "    return -numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def fletcher_reeves(X0, epsilon=1e-4, max_iterations=100):\n",
    "    X = X0\n",
    "    for i in range(max_iterations):\n",
    "        gradient = gradient_f(*X)\n",
    "        if i == 0:\n",
    "            S = -gradient\n",
    "        else:\n",
    "            beta = np.dot(gradient, gradient) / np.dot(gradient_prev, gradient_prev)\n",
    "            S = -gradient + beta * S_prev\n",
    "        lambda_star = optimal_step_length(X, S)\n",
    "        X_new = X + lambda_star * S\n",
    "        print(\"Iteration\", i + 1)\n",
    "        print(\"Gradient of f:\", gradient)\n",
    "        print(\"Search direction S:\", S)\n",
    "        print(\"X:\", X_new)\n",
    "        print(\"Lambda:\", lambda_star)\n",
    "        if i == 1 or np.linalg.norm(X_new - X) < epsilon:\n",
    "            break\n",
    "        else:\n",
    "            gradient_prev = gradient\n",
    "            S_prev = S\n",
    "            X = X_new\n",
    "    return X_new\n",
    "\n",
    "# Initial point\n",
    "X0 = np.array([1, 2])\n",
    "\n",
    "# Perform optimization\n",
    "optimal_solution = fletcher_reeves(X0)\n",
    "print(\"Optimal solution:\", optimal_solution)\n",
    "print(\"Minimum value of the objective function:\", f(*optimal_solution))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
